{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akdev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text Summarizer | Type = Extractive\n",
    "# Using Term Frequency - Inverse Document Frequency Technique\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print('punkt')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print('stopwords')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(text):\n",
    "    punctuation_marks = dict((ord(punctuation_mark), None)\n",
    "                             for punctuation_mark in string.punctuation)\n",
    "    # new_dict = [key for key in punctuation_marks.keys if punctuation_marks[key] != None]\n",
    "    return text.translate(punctuation_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_tokens(text):\n",
    "    normalized_tokens = nltk.word_tokenize(\n",
    "        remove_punctuation_marks(text.lower()))\n",
    "    return [nltk.stem.WordNetLemmatizer().lemmatize(normalized_token) for normalized_token in normalized_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(values):\n",
    "    greater_than_zero_count = 0\n",
    "    total = 0\n",
    "    for value in values:\n",
    "        if value != 0:\n",
    "            greater_than_zero_count += 1\n",
    "            total += value\n",
    "    if total == 0 and greater_than_zero_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total / greater_than_zero_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(tfidf_results):\n",
    "    i = 0\n",
    "    total = 0\n",
    "    while i < (tfidf_results.shape[0]):\n",
    "        total += get_average(tfidf_results[i, :].toarray()[0])\n",
    "        i += 1\n",
    "    return total / tfidf_results.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(documents, tfidf_results, handicap=0.85):\n",
    "    summary = \"\"\n",
    "    i = 0\n",
    "    while i < (tfidf_results.shape[0]):\n",
    "        if (get_average(tfidf_results[i, :].toarray()[0])) >= get_threshold(tfidf_results) * handicap:\n",
    "            summary += ' ' + documents[i]\n",
    "        i += 1\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use below function for calling from different file | Pass Text and handicap and then get Summary\n",
    "def summary_func(text=\"\", handicap=0.9):\n",
    "    documents = nltk.sent_tokenize(text)\n",
    "\n",
    "    tfidf_results = TfidfVectorizer(tokenizer=get_lemmatized_tokens, stop_words=stopwords.words(\n",
    "        'english')).fit_transform(documents)\n",
    "    \n",
    "    return get_summary(documents, tfidf_results, handicap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GTA 5 on this laptop. Nice product. Nothing great to talk about! The sound has a lot more to improve. But I believe there are better offers. Expecting a response at the earliest. Just Go For It Dont Think. The best laptop for students.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        text = '''This product is good for students for Study You can not play heavy gamesike? GTA 5 on this laptop.\n",
    "              Nice product. good for students but can improve in screen resolution?.\n",
    "              Quite dissatisfied with the screen quality overall it's all fine?.\n",
    "              Nothing great to talk about! The specs are mediocre and the screen quality is passable.\n",
    "              The sound has a lot more to improve.\n",
    "              The size is quite ok for a satchel/ backpack and is lightweight!\n",
    "              But I believe there are better offers.\n",
    "              Good for light personal use only like browsing and office work!\n",
    "              ASUS VIVOBOOK15 (2021)CELERON N4020, is not adequate for our office purpose we want to return it or exchange it for one with i5 or i3 processor.\n",
    "              need to know where to send this bought item for refund or exchange.\n",
    "              Expecting a response at the earliest. Just Go For It Dont Think. The best laptop for students.'''\n",
    "\n",
    "        documents = nltk.sent_tokenize(text)\n",
    "        # print(documents)\n",
    "        # print([get_lemmatized_tokens(d) for d in documents])\n",
    "\n",
    "        stop_words_list = stopwords.words('english').extend([get_lemmatized_tokens(i) for i in stopwords.words('english')]) \n",
    "        tfidf_results = TfidfVectorizer(tokenizer=get_lemmatized_tokens, stop_words=stop_words_list).fit_transform(\n",
    "            documents)\n",
    "        # nxm vector output n rows =  num_sentences or docs\n",
    "        # nxm vector output m cols =  num_tokens\n",
    "        # print(tfidf_results)\n",
    "        # Handicap Parameter = 0.9\n",
    "        print(get_summary(documents, tfidf_results, 0.9))\n",
    "        # Length of Summarized text is inversely prop. to Handicap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.47860244 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3568927\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_results[0:5, 0:7].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.47860244 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3568927\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.47132396 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.28122944]\n",
      " [0.         0.         0.31224164 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.25135036 0.         0.         0.\n",
      "  0.25135036]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.25629645]\n",
      " [0.20168016 0.         0.         0.         0.20168016 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.34959771 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_results[:, 0:7].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26642928\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3187853  0.30593331 0.         0.23840072 0.         0.\n",
      "  0.         0.30593331 0.         0.         0.         0.\n",
      "  0.         0.21666008 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26642928 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30593331\n",
      "  0.         0.26642928 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23840072 0.30593331 0.\n",
      "  0.         0.         0.         0.23840072 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30593331]\n",
      " [0.         0.47860244 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.47860244\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.41680228 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.47860244 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37295437 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.75411672 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.65674042 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.34509515 0.34509515\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2064549  0.         0.         0.30879088 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.34509515\n",
      "  0.39626314 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.39626314 0.         0.         0.         0.30879088 0.\n",
      "  0.         0.         0.         0.30879088 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3568927\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3568927  0.         0.         0.         0.         0.3568927\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27811118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.3568927  0.         0.         0.\n",
      "  0.         0.         0.         0.31080846 0.31080846 0.\n",
      "  0.         0.         0.         0.         0.27811118 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.21450654 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.31080846 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_results[0:5, :].toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
